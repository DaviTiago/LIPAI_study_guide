{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b543e158",
   "metadata": {},
   "source": [
    "# Semana 7 - Onboarding LIPAI\n",
    "## Redes Neurais Artificiais com PyTorch\n",
    "\n",
    "### Respostas dos exercícios propostos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dec2bc",
   "metadata": {},
   "source": [
    "## 1 - Com base no material apresentado no notebook, o que é uma função de ativação (como a ReLU)? Por que normalmente usamos entre as camadas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab020b5",
   "metadata": {},
   "source": [
    "Uma função de ativação é uma função matemática aplicada à saída de cada neurônio em uma rede neural. Ela transforma a soma ponderada das entradas, determinando a saída final do neurônio e se ele deve ser \"ativado\" para passar informação para a próxima camada. \n",
    "A função ReLU (Rectified Linear Unit), por exemplo, simplesmente retorna o valor de entrada se ele for positivo, e zero caso contrário (`f(x) = max(0, x)`).\n",
    "\n",
    "A principal razão para usar funções de ativação (especialmente as não-lineares, como a ReLU) é introduzir a **não linearidade** no modelo. Sem elas, uma rede neural, por mais profunda que fosse, se comportaria como um simples modelo de regressão linear, pois a composição de múltiplas transformações lineares resulta em outra transformação linear. A não linearidade permite que a rede aprenda e represente relações muito mais complexas e sofisticadas entre os dados de entrada e saída, o que é essencial para a maioria das tarefas do mundo real, como reconhecimento de imagem e processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4c90f",
   "metadata": {},
   "source": [
    "## 2 -  Explique o que cada uma das seguintes linhas de código faz e por que ela é necessária\n",
    "1. model.train()\n",
    "2. optimizer.step()\n",
    "3. Qual a diferença fundamental entre os modos model.train() e model.eval()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406990b",
   "metadata": {},
   "source": [
    "1. `model.train()`: Esta linha coloca o modelo em modo de treinamento. Isso é importante porque algumas camadas, como Dropout e BatchNorm, se comportam de maneira diferente durante o treinamento e a avaliação. No modo de treinamento, o Dropout desativa aleatoriamente algumas unidades para evitar overfitting, zerando valores de algun neurônios durante o treinamento. Já o BatchNorm normaliza os dados com base nas estatísticas do batch atual. Portanto, é crucial chamar `model.train()` antes de iniciar o processo de treinamento para garantir que essas camadas funcionem corretamente. \n",
    "\n",
    "2. optimizer.setp(): : Ela atualiza os parâmetros do modelo (pesos e vieses) com base nos gradientes calculados durante a retropropagação (loss.backward()). O algoritmo de otimização específico usado determina como essa atualização é feita, usando a taxa de aprendizado e outros hiperparâmetros definidos no otimizador, ultilizando gradientes para ajustar os parametros na direção que minimiza a função de perda.\n",
    "\n",
    "3. Qual a diferença fundamental entre os modos model.train() e model.eval()?\n",
    "model.train(): Este modo coloca o modelo em modo de treinamento. Durante o treinamento, certas camadas se comportam de maneira diferente. Por exemplo, camadas como Dropout e BatchNorm (se houver) estão ativas e se comportam como esperado durante o treinamento (aplicando dropout aleatório ou atualizando estatísticas de normalização em lote). Além disso, o PyTorch rastreia os gradientes das operações no modo de treinamento, o que é necessário para a retropropagação.\n",
    "\n",
    "model.eval(): Este modo coloca o modelo em modo de avaliação. Em contraste com o modo de treinamento, camadas como Dropout são desativadas (não aplicam dropout) e BatchNorm usa as estatísticas médias e de variância aprendidas durante o treinamento (em vez de calcular novas estatísticas para cada lote de avaliação). O PyTorch também desativa o cálculo de gradientes por padrão no modo de avaliação para economizar memória e computação, pois os gradientes não são necessários para a inferência."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6301a",
   "metadata": {},
   "source": [
    "## 3 - Modifique a classe ClassBin para que a rede tenha a seguinte arquitetura:\n",
    "1. Camada de Entrada: Mantém as 4 features de entrada.\n",
    "2. Primeira Camada Oculta: nn.Linear com 4 neurônios de entrada e 16 neurônios de saída, seguida por uma ativação ReLU.\n",
    "3. Segunda Camada Oculta: nn.Linear com 16 neurônios de entrada e 8 neurônios de saída, seguida por uma ativação ReLU.\n",
    "4. Camada de Saída: nn.Linear com 8 neurônios de entrada e 1 neurônio de saída, seguida por uma ativação Sigmoid.\n",
    "5. Remova todas as camadas de Dropout para uma nova arquitetura.\n",
    "6. Treine o novo modelo com os mesmos hiperparâmetros (épocas, taxa de aprendizado, etc.) e compare a acurácia final (de treino e teste) com a do modelo original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9c807",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ClassBin2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassBin, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(4, 16) # primeira hidden layer  com 4 neurônios de entrada e 16 neurônios de saída\n",
    "        self.linear2 = nn.Linear(16, 8) # segunda hidden layer # segunda hidden layer\n",
    "        self.linear3 = nn.Linear(8, 1)  # terceira hidden layer om 8 neurônios de entrada e 1 neurônio de saída  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.ReLu = nn.ReLU()\n",
    "\n",
    "    # Propagação (Feed Forward)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ClassBin()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4c2e1",
   "metadata": {},
   "source": [
    "**Modelo Clássico:**\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    " \n",
    "Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "\n",
    "**Modelo 2 - EX3:**\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d68a30",
   "metadata": {},
   "source": [
    "## 4 -  Usando o modelo original do notebook:\n",
    "\n",
    "1. Mude o Otimizador: Substitua o otimizador **Adam** por SGD (Stochastic Gradient Descent). \n",
    "2. Treine o modelo com o SGD.\n",
    "3. O que aconteceu com o custo (loss) durante o treinamento? A acurácia final foi melhor ou pior? O SGD com essa taxa de aprendizado pareceu uma boa escolha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f3c84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "epochs = 100\n",
    "batch_size = 32  # X_train 535 / 32 = 16.71 (então são 17 batches de 32)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Instânciar o Otimizador Adam\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516dd56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Época 1,\n",
    "          Custo Treino: 0.693\n",
    "Época 2,\n",
    "          Custo Treino: 0.692\n",
    "Época 3,\n",
    "          Custo Treino: 0.693\n",
    "Época 4,\n",
    "          Custo Treino: 0.697\n",
    "Época 5,\n",
    "          Custo Treino: 0.669\n",
    "Época 6,\n",
    "          Custo Treino: 0.675\n",
    "Época 7,\n",
    "          Custo Treino: 0.691\n",
    "Época 8,\n",
    "          Custo Treino: 0.779\n",
    "Época 9,\n",
    "          Custo Treino: 0.681\n",
    "Época 10,\n",
    "          Custo Treino: 0.693"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0592c",
   "metadata": {},
   "source": [
    "**Adam:**\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    " \n",
    "Acurácia de Teste: 0.6033519506454468\n",
    "\n",
    "**SGD:**\n",
    "\n",
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62fdb6",
   "metadata": {},
   "source": [
    "Com SGD o custo de treinamento variou até a 10 epoch logo após tornou-se constante, já a acurácia continuou igual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68eda8d",
   "metadata": {},
   "source": [
    "## 5 - Usando o modelo original e o otimizador Adam:\n",
    "\n",
    "1. No DataLoader, mude o batch_size de 32 para um valor muito maior, como 512.\n",
    "2. Treine o modelo e observe a acurácia.\n",
    "3. Agora, faça o oposto. Mude o batch_size para um valor bem pequeno, como 4, e treine novamente.\n",
    "4. Como a mudança no batch_size afetou a estabilidade do custo (loss) a cada época e a acurácia final do modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5ddf1",
   "metadata": {},
   "source": [
    "***Batch_size 512***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab8a54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "epochs = 100\n",
    "batch_size = 512 # X_train 535 / 32 = 16.71 (então são 17 batches de 32)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Instânciar o Otimizador Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1d766",
   "metadata": {},
   "source": [
    "O custo Loss variou apena na primeira epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c50a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Época 1,\n",
    "          Custo Treino: 0.853\n",
    "Época 2,\n",
    "          Custo Treino: 0.693\n",
    "Época 3,\n",
    "          Custo Treino: 0.693"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae07748",
   "metadata": {},
   "source": [
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7705ada8",
   "metadata": {},
   "source": [
    "***Batch_size 4***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd334dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "epochs = 100\n",
    "batch_size = 4 # X_train 535 / 32 = 16.71 (então são 17 batches de 32)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Instânciar o Otimizador Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b187b9b",
   "metadata": {},
   "source": [
    "O custo Loss não variou, mas o treinamento demorou muito mais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507b00d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Época 1,\n",
    "          Custo Treino: 0.693\n",
    "Época 2,\n",
    "          Custo Treino: 0.693\n",
    "Época 3,\n",
    "          Custo Treino: 0.693\n",
    "Época 4,\n",
    "          Custo Treino: 0.693\n",
    "Época 5,\n",
    "          Custo Treino: 0.693\n",
    "Época 6,\n",
    "          Custo Treino: 0.693\n",
    "Época 7,\n",
    "          Custo Treino: 0.693\n",
    "Época 8,\n",
    "          Custo Treino: 0.693\n",
    "Época 9,\n",
    "          Custo Treino: 0.693\n",
    "Época 10,\n",
    "          Custo Treino: 0.693\n",
    "..........\n",
    "Época 99,\n",
    "          Custo Treino: 0.693\n",
    "Época 100,\n",
    "          Custo Treino: 0.693"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d652a",
   "metadata": {},
   "source": [
    "Também não houve alteração na Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b402f33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Acurácia de Treino: 0.590654194355011\n",
    "\n",
    " ---------------------------\n",
    "\n",
    "Acurácia de Teste: 0.6033519506454468"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
